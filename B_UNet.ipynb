{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "import DA_core as DA\n",
    "from glob import glob\n",
    "import torch.utils.data as Data\n",
    "from torch import optim\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import ML_core as ML\n",
    "from numpy.random import default_rng\n",
    "import os\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "rng = default_rng()\n",
    "DA.read_data_dir='/scratch/cimes/feiyul/PyQG/data/training'\n",
    "DA.save_data_dir='/scratch/cimes/feiyul/PyQG/data/training'\n",
    "\n",
    "data_dir='/scratch/cimes/feiyul/PyQG/data'\n",
    "# data_dir='/net2/fnl/PyQG/data'\n",
    "B_ens_kws={'cmap':'bwr','levels':np.linspace(-2.5E-11,2.5E-11,26),'extend':'both'}\n",
    "B_ens_kws1={'cmap':'bwr','levels':np.linspace(-0.25E-11,0.25E-11,26),'extend':'both'}\n",
    "q_kws={'cmap':'bwr','levels':np.linspace(-3.4E-5,3.4E-5,18),'extend':'both'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DA_paras={'nens':20,\n",
    "          'DA_method':'EnKF',\n",
    "          'Nx_DA':32,\n",
    "          'Nx_truth':128,\n",
    "          'obs_freq':10,\n",
    "          'obs_err':[1,-5,5,-7],\n",
    "          'DA_freq':10,\n",
    "          'save_B':False,\n",
    "          'nobs':[10,10],\n",
    "          'R_W':150,\n",
    "          'inflate':[1,0.5]}\n",
    "DA_exp=DA.DA_exp(**DA_paras)\n",
    "print(DA_exp.file_name())\n",
    "# obs_ds=DA_exp.read_obs()\n",
    "in_ch=[0,1]\n",
    "out_ch=[0,1,2]\n",
    "\n",
    "DA_paras1={'nens':320,\n",
    "          'DA_method':'EnKF',\n",
    "          'Nx_DA':64,\n",
    "          'Nx_truth':128,\n",
    "          'obs_freq':10,\n",
    "          'obs_err':[1,-6,2,-8],\n",
    "          'DA_freq':10,\n",
    "          'save_B':False,\n",
    "          'nobs':[100,0],\n",
    "          'R_W':100,\n",
    "          'inflate':[1,0.55]}\n",
    "DA_exp1=DA.DA_exp(**DA_paras1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make direcotry for storing networks\n",
    "os.makedirs('./ML/{}'.format(DA_exp.file_name()),exist_ok=True)\n",
    "\n",
    "### Find time indices for the DA steps to select proper q and B data\n",
    "DA_days=slice(729,1460,DA_exp.DA_freq)\n",
    "DA_it=slice(int((DA_days.start-DA_exp.DA_freq+1)/DA_exp.DA_freq),int((DA_days.stop-DA_exp.DA_freq+1)/DA_exp.DA_freq)+1)\n",
    "print(DA_days,DA_it)\n",
    "\n",
    "### range of indices to select from q and B data\n",
    "i_x=slice(0,DA_exp.Nx_DA)\n",
    "i_y=slice(0,DA_exp.Nx_DA)\n",
    "\n",
    "### size and starting index for the B data in U-Nets\n",
    "B_size=20\n",
    "B_start=0\n",
    "\n",
    "### Read the saved covariance matrices from previous EnKF experiments\n",
    "B_ens_ds=xr.open_dataset('{}/training/{}/B_ens.nc'.format(data_dir,DA_exp.file_name()))\n",
    "B_ens=B_ens_ds.B_ens.isel(time=DA_it,y=i_y,x=i_x)\n",
    "print(B_ens.shape)\n",
    "\n",
    "### Read the saved ensemble-mean analysis q from previous EnKF experiments\n",
    "mean_ds=DA_exp.read_mean().load()\n",
    "q_full=mean_ds.q.isel(time=DA_days,y=i_y,x=i_x)\n",
    "print(q_full.shape)\n",
    "\n",
    "### Read or calculate standard deviations for normalization\n",
    "if os.path.exists('./ML/{0}/std_{0}.nc'.format(DA_exp.file_name())):\n",
    "    ml_std_ds=xr.open_dataset('./ML/{0}/std_{0}.nc'.format(DA_exp.file_name()))\n",
    "else:\n",
    "    B_std=np.empty((2,2))\n",
    "    B_std[0,0]=np.std(B_ens.isel(lev=0,lev_d=0))\n",
    "    B_std[0,1]=np.std(B_ens.isel(lev=0,lev_d=1))\n",
    "    B_std[1,0]=B_std[0,1]\n",
    "    B_std[1,1]=np.std(B_ens.isel(lev=1,lev_d=1))\n",
    "\n",
    "    q_std=np.zeros((2,1))\n",
    "    q_std[0]=np.std(q_full.isel(time=DA_days,lev=0))\n",
    "    q_std[1]=np.std(q_full.isel(time=DA_days,lev=1))\n",
    "\n",
    "    ml_std_ds=xr.Dataset({'B_std':xr.DataArray(B_std,coords=[mean_ds.lev,mean_ds.lev]),\n",
    "                          'q_std':xr.DataArray(q_std.squeeze(),coords=[mean_ds.lev])})\n",
    "    ml_std_ds.to_netcdf('./ML/{0}/std_{0}.nc'.format(DA_exp.file_name()))    \n",
    "print(ml_std_ds)\n",
    "\n",
    "### Process B data for training\n",
    "B_stacked=B_ens.stack(sample=('time','y','x')).transpose('sample',...)\n",
    "print(B_stacked.shape)\n",
    "\n",
    "B_data=np.empty((len(B_ens.time)*len(B_ens.y)*len(B_ens.x),3,len(B_ens.y_d),len(B_ens.x_d)))\n",
    "B_data[:,0,...]=B_stacked[:,0,0,...]/ml_std_ds.B_std[0,0].data\n",
    "B_data[:,1,...]=B_stacked[:,0,1,...]/ml_std_ds.B_std[0,1].data\n",
    "B_data[:,2,...]=B_stacked[:,1,1,...]/ml_std_ds.B_std[1,1].data\n",
    "print(B_data.shape)\n",
    "\n",
    "### Process q data for training\n",
    "q_local=np.empty((len(q_full.time),len(q_full.lev),len(q_full.y),len(q_full.x),len(B_ens.y_d),len(B_ens.x_d)))\n",
    "for i in range(len(q_full.x)):\n",
    "    for j in range(len(q_full.y)):\n",
    "        q_local[:,:,j,i,:,:]=DA.localize_q(q_full,j,i,DA_exp.Nx_DA,int(len(B_ens.x_d)/2))\n",
    "\n",
    "q_local=q_local.transpose([0,2,3,1,4,5])\n",
    "print(q_local.shape)\n",
    "q_data=q_local.reshape((len(q_full.time)*len(q_full.y)*len(q_full.x),len(q_full.lev),len(B_ens.y_d),len(B_ens.x_d)))\n",
    "print(q_data.shape)\n",
    "q_data[:,0,...]=q_data[:,0,...]/ml_std_ds.q_std[0].data\n",
    "q_data[:,1,...]=q_data[:,1,...]/ml_std_ds.q_std[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_unet=q_data[...,B_start:B_start+B_size,B_start:B_start+B_size]\n",
    "B_unet=B_data[...,B_start:B_start+B_size,B_start:B_start+B_size]\n",
    "B_shape=B_unet.shape\n",
    "q_shape=q_unet.shape\n",
    "print(B_shape,q_shape)\n",
    "\n",
    "n_total=B_shape[0]\n",
    "n_train=int(n_total*0.8)\n",
    "\n",
    "train_ds=ML.Dataset(q_unet[0:n_train,...],B_unet[0:n_train,...],device)\n",
    "valid_ds=ML.Dataset(q_unet[n_train:,...],B_unet[n_train:,...],device)\n",
    "\n",
    "params = {'batch_size':16,'num_workers':1,'shuffle':True}\n",
    "training_generator = torch.utils.data.DataLoader(train_ds, **params)\n",
    "validation_generator = torch.utils.data.DataLoader(valid_ds, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=ML.Unet_2L(in_ch=len(in_ch),out_ch=len(out_ch))\n",
    "model=model.to(device)\n",
    "print(device)\n",
    "\n",
    "# check keras-like model summary using torchsummary\n",
    "# summary(model, input_size=q_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() # MSE loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.double()\n",
    "n_epochs = 2 #Number of epocs\n",
    "validation_loss = list()\n",
    "train_loss = list()\n",
    "start_epoch=0\n",
    "if start_epoch>0:\n",
    "    model_file='./ML/unet_epoch{}_in{}_out{}_B{}_{}.pt'.format(start_epoch,''.join(map(str,in_ch)),''.join(map(str,out_ch)),B_size,DA_exp.file_name())\n",
    "    print(model_file)\n",
    "    model.load_state_dict(torch.load(model_file,map_location=torch.device('cpu')))\n",
    "# time0 = time()  \n",
    "for epoch in range(start_epoch+1, n_epochs + 1):\n",
    "    train_loss.append(ML.train_model(model,criterion,training_generator,optimizer,device))\n",
    "    # validation_loss.append(ML.test_model(model,criterion,validation_generator,optimizer,device))\n",
    "    torch.save(model.state_dict(), './ML/{}/unet_epoch{}_in{}_out{}_B{}_{}.pt'.\\\n",
    "        format(DA_exp.file_name(),epoch,''.join(map(str,in_ch)),''.join(map(str,out_ch)),B_size,DA_exp.file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss,'b', label='training loss');\n",
    "plt.plot(validation_loss,'r', label='validation loss');\n",
    "\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyqg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6973bb065886c44b729be95666fc7914cdffcda5c95a8d198f96362135dd92cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
